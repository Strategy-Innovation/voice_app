{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7dfd27a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [23804]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:52139 - \"GET / HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:52139 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:52139 - \"GET /basic HTTP/1.1\" 200 OK\n",
      "Check\n",
      "\n",
      "<class 'str'>\n",
      "Check1234\n",
      "\n",
      "\n",
      "ICICI PRRS SYSTEM\n",
      "/Users/strategy_innovation/Desktop/Chatgpt\\New Recording.m4a.m4a\n",
      "<class 'list'>\n",
      "Does Client XYZ have closed loop feeder system?\n",
      "Yes\n",
      "Is clients storage facility good?\n",
      "No\n",
      "Does Client have work permit systemDoes Client conduct fire mock drill\n",
      "Yes, Client\n",
      "Does client have firehydrant system\n",
      "Yes\n",
      "Does client have fire extinguisher?\n",
      "Yes\n",
      "Does client have fire sprinkler system\n",
      "No\n",
      "Does client follow the industry compliances?\n",
      "No\n",
      "Are the employees good?\n",
      "Yes\n",
      "1\n",
      "\n",
      "1\n",
      "[' ', 'Yes', 'No', 'Yes, Client', 'Yes', 'Yes', 'No', 'No', 'Yes']\n",
      "Q: Does Client XYZ have closed loop feeder system?\n",
      "A: Yes\n",
      "Q: Is clients storage facility good?\n",
      "A: No\n",
      "Q: Does Client have work permit systemDoes Client conduct fire mock drill\n",
      "A: Yes, Client\n",
      "Q: Does client have firehydrant system\n",
      "A: Yes\n",
      "Q: Does client have fire extinguisher?\n",
      "A: Yes\n",
      "Q: Does client have fire sprinkler system\n",
      "A: No\n",
      "Q: Does client follow the industry compliances?\n",
      "A: No\n",
      "Q: Are the employees good?\n",
      "A: Yes\n",
      "Client Score\n",
      "\n",
      "44.44\n",
      "Chemical\n",
      "<class 'list'>\n",
      "2\n",
      "\n",
      "105\n",
      "INFO:     127.0.0.1:52140 - \"POST /api/Upload HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import openai\n",
    "openai.api_key = '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'\n",
    "#openai.api_key = 'qGc1sk-AQoz3EcUXuUk9tOnMcDlT3BlbkFJb0xJL6EYMhFFoCwQCiX5'\n",
    "#openai.api_key='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'\n",
    "import openpyxl\n",
    "from flask import Flask\n",
    "from flask_cors import CORS\n",
    "#import request\n",
    "import docx\n",
    "import time\n",
    "import nltk\n",
    "#import azure.cognitiveservices.speech as speechsdk\n",
    "import time\n",
    "#from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from fastapi import FastAPI, File, UploadFile\n",
    "from fastapi.responses import FileResponse\n",
    "from fastapi import FastAPI, Request, Form, UploadFile, File\n",
    "import openpyxl\n",
    "import uvicorn\n",
    "from scipy.io.wavfile import write\n",
    "#import wavio as wv\n",
    "#from fuzzywuzzy import process,fuzz\n",
    "#from pydub import AudioSegment\n",
    "import math\n",
    "app = Flask(__name__)\n",
    "CORS(app)\n",
    "import pandas as pd\n",
    "from nltk.tokenize import  word_tokenize, sent_tokenize\n",
    "\n",
    "from fastapi.responses import HTMLResponse\n",
    "from fastapi.staticfiles import StaticFiles\n",
    "from fastapi.templating import Jinja2Templates\n",
    "\n",
    "#API_KEY = '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' \n",
    "#content1=\"Their storage tank farms are very congested. Client do not conduct mock drill. Answer below questions from above information. If answer to any question below is uncertain, please respond Uncertain, otherwise respond Yes or No without ,.\" \n",
    "instruction=\"Please respond the first question with occupancy If you are not sure about answer to the question, please respond Uncertain, otherwise respond 'Yes' or 'No'\"\n",
    "answer_check = 0\n",
    "#questions = [     \"Does client have fire sprinkler system\",     \"Does client follow the industry compliances?\",  \"Are the employees good?\" ]\n",
    "question1= \"Please respond in one word without any punctuation the following question, What is the occupancy?\"\n",
    "questions = [\n",
    "    \"Does Client XYZ have closed loop feeder system?\",\n",
    "    \"Is clients storage facility good?\",\n",
    "    \"Does Client have work permit system\"\n",
    "    \"Does Client conduct fire mock drill\",\n",
    "    \"Does client have firehydrant system\",\n",
    "    \"Does client have fire extinguisher?\",\n",
    "    \"Does client have fire sprinkler system\",\n",
    "    \"Does client follow the industry compliances?\",\n",
    "    \"Are the employees good?\"\n",
    "    \n",
    "    # Add more questions as needed\n",
    "    ]\n",
    "def text_conerter(file_path):\n",
    "    model_id=\"whisper-1\"\n",
    "    #media_file_path=\"C:\\\\Users\\\\PRV\\\\Recording.m4a\"\n",
    "    #media_file_path=\"C:\\\\Users\\\\PRV\\\\Documents\\\\Sound Recordings\\\\m_test.m4a\"\n",
    "    media_file_path =file_path\n",
    "    print(media_file_path)\n",
    "    media_file=open(media_file_path,\"rb\")\n",
    "    response=openai.Audio.transcribe(api_key=openai.api_key, model=model_id,file=media_file)\n",
    "    content1=response.text + \"\\n\" + str(instruction) + \"\\n\"\n",
    "    return content1\n",
    "\n",
    "def store_file_get_link(uploaded_file):\n",
    "    #wv.write(uploaded_file.filename,uploaded_file, 44100, sampwidth=2)\n",
    "    file_location = os.getcwd()+\"\\\\\"+uploaded_file.filename+\".m4a\"\n",
    "    print(\"\\nICICI PRRS SYSTEM\")\n",
    "    with open(file_location, \"wb+\") as file_object:\n",
    "        file_object.write(uploaded_file.file.read())\n",
    "    return (file_location)\n",
    "\n",
    "#content= str(text_conerter()) + \"\\n\" + str(instruction) + \"\\n\"\n",
    "\n",
    "def get_openai_answer(question, context):\n",
    "    # Call the OpenAI API to generate an answer\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-003\",  # Choose the appropriate engine\n",
    "        prompt=context + \"\\nQ: \" + question + \"\\nA:\",\n",
    "        max_tokens=3,  # Adjust the number of tokens as per your needs\n",
    "        temperature=0.3,  # Adjust the temperature value as per your needs\n",
    "        n=1,  # Number of responses to generate\n",
    "        stop=None,  # Stop sequence to limit the response length (optional)\n",
    "    )    \n",
    "    # Extract the answer from the API response\n",
    "    answer = response.choices[0].text.strip().split(\"\\n\")[0]\n",
    "    return answer\n",
    "\n",
    "def generate_question_from_summary(summary):\n",
    "    #prompt = \"Below summary explains cause of the claims for an insurance company in an occupancy. Based on the cause of claims, I want to generate questions which can be used in the future by the insurance companies to inspect new clients who belong to similar industry. The questions should be framed such that it can be answered in Yes or No. Please note that I already ha questions should be useful to predict the chance of the claim occuring in future for new clients. In case no such question can be genrated which helps to predict the likihood of claims in future for similar client, don't forecefully give questions.  :\\n\"+summary\n",
    "    prompt=\"Below is a summary of losses due to fire and it might contain the root cause for the occurence of a fire loss in chemical occupancy. I need to use the generate questions which can be answered in Yes or No from the loss summary related to the cause of loss which would be helpful for assessing similar clients in the future. \\n\"\n",
    "    response = openai.Completion.create(\n",
    "        engine=\"text-davinci-002\",\n",
    "        prompt=prompt,\n",
    "        temperature=0.2,\n",
    "        max_tokens=350,\n",
    "    )\n",
    "    question = response['choices'][0]['text'].strip()\n",
    "    return question\n",
    "\n",
    "def new_question_generation(industry):\n",
    "    file_path = \"PRRS_Claims_Data.xlsx\"\n",
    "    sheet_name = 'Sheet1'\n",
    "    summary_column = 'Loss_Remarks'\n",
    "    target_columns = 'Industry_Type'\n",
    "    new_question=[]\n",
    "    new_question1=[]\n",
    "    question=\"\"\n",
    "    j=0\n",
    "    df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "    for _, row in df.iterrows():\n",
    "        summary = row[summary_column]\n",
    "        occupancy=row[target_columns]\n",
    "        #print(occupancy)\n",
    "        #print(\"Summary\\n:\", summary)\n",
    "        if occupancy==industry:\n",
    "            j=j+1\n",
    "            question=generate_question_from_summary(summary)\n",
    "            question=question.split(\"\\n\")\n",
    "            for i in question:\n",
    "                h=i+\" \"+str(j)\n",
    "                new_question.append(h)\n",
    "    for item in new_question:\n",
    "        if item and item[0].isdigit():\n",
    "            new_question1.append(item.split(' ',1)[1]) \n",
    "        else:\n",
    "            new_question1.append(item)\n",
    "    return(new_question1)\n",
    "\n",
    "\n",
    "def answer_validation(ans_response,questions,context):\n",
    "    unanswered_questions=[]\n",
    "    answered_questions=[]\n",
    "    answer_responses=ans_response.split(\",\")\n",
    "    a=len(questions)\n",
    "    flag=0\n",
    "    for question in questions:\n",
    "       # time.sleep(21)\n",
    "        answer = get_openai_answer(question, context)\n",
    "        print(question)\n",
    "        print(answer)\n",
    "        if answer==\"Uncertain\" or answer==\"Uncertain.\" or answer==\"Uncertain,\":\n",
    "            unanswered_questions.append(question)\n",
    "        else:\n",
    "            answered_questions.append(question)\n",
    "            answer_responses.append(answer)\n",
    "            flag=flag+1\n",
    "    if flag<a:\n",
    "        return (0,unanswered_questions,answered_questions,answer_responses)\n",
    "    else:\n",
    "        return (1,unanswered_questions,answered_questions,answer_responses)\n",
    "def score_generation(answered_score,questions,context):\n",
    "    count=0\n",
    "    score=0\n",
    "    a=0\n",
    "    answered_score=answered_score.split(\",\")    \n",
    "    for question in questions:\n",
    "        #time.sleep(30)\n",
    "        answer = get_openai_answer(question, context)\n",
    "        answered_score.append(answer)\n",
    "        print(\"Q: \" + question)\n",
    "        print(\"A: \" + answer)\n",
    "    for answer in answered_score:\n",
    "        count=count+1\n",
    "        if answer==\"Yes\":\n",
    "            score=score+100\n",
    "        else:\n",
    "            score=score+0\n",
    "    net_score= score/count\n",
    "    net_score= round(net_score,2)\n",
    "    print(\"Client Score\\n\")\n",
    "    print(net_score)\n",
    "    occupancy=get_openai_answer(question1,context)\n",
    "    b=new_question_generation(occupancy)\n",
    "    workbook=openpyxl.Workbook()\n",
    "    sheet=workbook.active\n",
    "    # Let's treat the array as a single column\n",
    "    for row_idx, cell_value in enumerate(b, start=1):\n",
    "    # Convert row index to Excel-style cell reference (e.g., A1, A2, A3, etc.)\n",
    "        cell = sheet.cell(row=row_idx, column=1)\n",
    "        cell.value = cell_value\n",
    "    workbook.save(\"claims_question.xlsx\")\n",
    "    print(occupancy)\n",
    "    #print(b)\n",
    "    return(net_score,b) \n",
    "\n",
    "def corefunction(ans_response,content2,L_content ):\n",
    "    L_questions=content2\n",
    "    flag,answer_check,answered_questions,answer_responses= answer_validation(ans_response,L_questions,L_content)\n",
    "    return (flag,answer_check,answered_questions,answer_responses)\n",
    "    \n",
    "import nest_asyncio\n",
    "nest_asyncio.apply()\n",
    "app=FastAPI()\n",
    "origins = [\"*\"]\n",
    "app.mount(\"/static\", StaticFiles(directory=\"template1\"), name=\"static\")\n",
    "\n",
    "templates = Jinja2Templates(directory=\"template1\")\n",
    "\n",
    "@app.get(\"/basic\", response_class=HTMLResponse)\n",
    "async def get_basic_form(request: Request):\n",
    "    response = answer_check\n",
    "#     return templates.TemplateResponse(\"home.html\", {\"request\": request, \"response\": response})\n",
    "    return templates.TemplateResponse(\"home_test1.html\", {\"request\": request, \"response\": response})\n",
    "\n",
    "@app.post('/api/Append')\n",
    "async def Click_Here1():\n",
    "    return({\"200 OK\"})\n",
    "    \n",
    "@app.post('/api/Upload')\n",
    "async def Click_Here(request: Request, myInput1: str = Form(...), myInput:  str = Form(...), file: UploadFile = File(...)):\n",
    "    unanswered_questions=myInput\n",
    "    temp=unanswered_questions\n",
    "    answered_score=myInput1\n",
    "    preious_score=answered_score\n",
    "    print(\"Check\\n\")\n",
    "    print(type(temp))\n",
    "    print(\"Check1234\\n\")\n",
    "    Wave_Path = store_file_get_link(file)\n",
    "    if unanswered_questions==\"ALL\":\n",
    "        inspection=text_conerter(Wave_Path)\n",
    "        L_content=inspection\n",
    "        L_questions=questions\n",
    "        print(type(L_questions))\n",
    "        answer_check,unanswered_questions,answered_questions,answer_responses= corefunction(answered_score,L_questions,L_content)\n",
    "        print(\"1\\n\")\n",
    "        print(answer_check)\n",
    "        print(answer_responses)\n",
    "        if answer_check==1:\n",
    "            score,b=score_generation(preious_score,L_questions,L_content)\n",
    "            print(type(b))\n",
    "            print(\"2\\n\")\n",
    "            new_questions=len(b)\n",
    "            print(new_questions)\n",
    "            return templates.TemplateResponse(\"New_App.html\", {\"request\": request, \"my_variable\": score, \"new_questions\":new_questions})\n",
    "#             return templates.TemplateResponse(\"New_App1.html\", {\"request\": request, \"my_variable\": score, \"new_questions\":new_questions})\n",
    "            # return({\"200 OK\":score})\n",
    "        else:\n",
    "            import json\n",
    "            return templates.TemplateResponse(\"New_App.html\", {\"request\": request, \"answer_responses\":json.dumps(answer_responses),\"unanswered\":unanswered_questions,\"unanswered_question\": json.dumps(unanswered_questions)})\n",
    "#             return templates.TemplateResponse(\"New_App1.html\", {\"request\": request, \"answer_responses\":json.dumps(answer_responses),\"unanswered\":unanswered_questions,\"unanswered_question\": json.dumps(unanswered_questions)})\n",
    "           # return({\"200 OK\":Wave_Path, \"Unanswered Question\":unanswered_questions, \"Answered Question\":answered_questions})\n",
    "    else:\n",
    "        inspection=text_conerter(Wave_Path)\n",
    "        L_content=inspection\n",
    "        L_questions=temp.split(\",\")\n",
    "        print(\" \\n\")\n",
    "        print(temp)\n",
    "        print(\" \\n\")\n",
    "        print(L_questions)\n",
    "        answer_check,unanswered_questions,answered_questions,answer_responses= corefunction(answered_score,L_questions,L_content)\n",
    "        print(\"Unanswered QUestion Check \\n\")\n",
    "        print(answer_check)\n",
    "        print(\"Test Question Check \\n\")\n",
    "        print(answered_questions)\n",
    "        print(\"Test1 Question Check \\n\")\n",
    "        if answer_check==1:\n",
    "            score,b=score_generation(preious_score,L_questions,L_content)\n",
    "            new_questions=len(b)\n",
    "            print(\"2\\n\")\n",
    "            print(b)\n",
    "            return templates.TemplateResponse(\"New_App.html\", { \"request\": request, \"my_variable\": score, \"new_questions\": new_questions})\n",
    "#             return templates.TemplateResponse(\"New_App1.html\", { \"request\": request, \"my_variable\": score, \"new_questions\": new_questions})\n",
    "        else:\n",
    "            import json\n",
    "            return templates.TemplateResponse(\"New_App.html\", {\"request\": request, \"answer_responses\":json.dumps(answer_responses),\"unanswered\":unanswered_questions,\"unanswered_question\": json.dumps(unanswered_questions)})\n",
    "#             return templates.TemplateResponse(\"New_App1.html\", {\"request\": request, \"answer_responses\":json.dumps(answer_responses),\"unanswered\":unanswered_questions,\"unanswered_question\": json.dumps(unanswered_questions)})\n",
    "if __name__==\"__main__\":\n",
    "        uvicorn.run(app)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13507993",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=2\n",
    "print(\"b\"+str(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6d6396",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"c\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c464b70a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35460f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6d091da",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(Path.cwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce2506da",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
